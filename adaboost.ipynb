{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alyci\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\alyci\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "x = np.load(\"data/x.npy\")\n",
    "y = np.load(\"data/y.npy\")\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "np.save(\"data/x_normalized.npy\", x_scaled)\n",
    "\n",
    "x_test = np.load(\"data/x_test.npy\")\n",
    "x_test_scaled = min_max_scaler.fit_transform(x_test)\n",
    "np.save(\"data/x_test_normalized.npy\", x_test_scaled)\n",
    "\n",
    "# 5fold\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alyci\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed: 573.5min\n",
      "[Parallel(n_jobs=2)]: Done 158 tasks      | elapsed: 675.3min\n",
      "[Parallel(n_jobs=2)]: Done 240 out of 240 | elapsed: 732.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Error: 0.779440\n",
      "Best Model: {'algorithm': 'SAMME.R', 'learning_rate': 0.3593813663804626, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': [50, 100, 500], 'algorithm':['SAMME.R', 'SAMME'],\n",
    "'learning_rate': np.logspace(-4, 0, 10)}\n",
    "\n",
    "adab = AdaBoostClassifier()\n",
    "clf = GridSearchCV(adab, scoring='accuracy', param_grid=params, n_jobs=2, verbose=2)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print('Best Error: ' + str(clf.best_score_))\n",
    "print('Best Model: ' + str(clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(algorithm='SAMME.R', learning_rate=0.3593813663804626, n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=0.3593813663804626, n_estimators=500,\n",
       "          random_state=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7839701857206922\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(x_scaled, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7907737655563116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "prob_preds = clf.predict_proba(x_scaled)\n",
    "performance = roc_auc_score(y, prob_preds[:, 1])\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv(model, x_data, name):\n",
    "    ids = [i for i, _ in enumerate(x_data)]\n",
    "    ids = np.array(ids)\n",
    "    \n",
    "    predictions = model.predict(x_data).flatten()\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['id'] = ids\n",
    "    df['target'] = predictions\n",
    "    df.to_csv(name + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_csv(clf, x_test_scaled, \"submissions/adaboost1\")\n",
    "# 0.54037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with rank: 1\n",
    "Mean validation score: 0.78258 (std: 0.00059)\n",
    "Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'max_features': 50, 'subsample': 0.9}\n",
    "\n",
    "Model with rank: 2\n",
    "Mean validation score: 0.78249 (std: 0.00123)\n",
    "Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'max_features': 100, 'subsample': 0.8}\n",
    "\n",
    "Model with rank: 3\n",
    "Mean validation score: 0.78142 (std: 0.00082)\n",
    "Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'max_features': 50, 'subsample': 0.8}\n",
    "\n",
    "Model with rank: 4\n",
    "Mean validation score: 0.78140 (std: 0.00116)\n",
    "Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'max_features': 20, 'subsample': 0.9}\n",
    "\n",
    "Model with rank: 5\n",
    "Mean validation score: 0.78062 (std: 0.00122)\n",
    "Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'max_features': 100, 'subsample': 0.9}\n",
    "\n",
    "Model with rank: 6\n",
    "Mean validation score: 0.78034 (std: 0.00102)\n",
    "Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'max_features': 20, 'subsample': 0.8}\n",
    "\n",
    "Model with rank: 7\n",
    "Mean validation score: 0.77956 (std: 0.00198)\n",
    "Parameters: {'learning_rate': 0.5, 'max_depth': 2, 'max_features': 50, 'subsample': 0.9}\n",
    "\n",
    "Model with rank: 8\n",
    "Mean validation score: 0.77935 (std: 0.00050)\n",
    "Parameters: {'learning_rate': 0.5, 'max_depth': 2, 'max_features': 100, 'subsample': 0.9}\n",
    "\n",
    "Model with rank: 9\n",
    "Mean validation score: 0.77905 (std: 0.00084)\n",
    "Parameters: {'learning_rate': 0.5, 'max_depth': 2, 'max_features': 20, 'subsample': 0.9}\n",
    "\n",
    "Model with rank: 10\n",
    "Mean validation score: 0.77893 (std: 0.00104)\n",
    "Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'max_features': 20, 'subsample': 0.9}\n",
    "\n",
    "Best Error: 0.782579\n",
    "Best Model: {'learning_rate': 0.1, 'max_depth': 5, 'max_features': 50, 'subsample': 0.9}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
