{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import random as rn\n",
    "\n",
    "np.random.seed(42)\n",
    "rn.seed(42)\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "generate_data = False\n",
    "\n",
    "\n",
    "x = np.load(\"data/x.npy\")\n",
    "y = np.load(\"data/y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "\n",
    "best_aucroc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = ['relu']\n",
    "dropouts = [0.8]\n",
    "num_total_layers = [2]\n",
    "losses = ['binary_crossentropy']\n",
    "val_sizes = [0.33]\n",
    "last_layer_sizes = [100]\n",
    "last_layer_drop_sizes = [0.15]\n",
    "\n",
    "\n",
    "# activations = ['tanh']\n",
    "# dropouts = [0.93, 0.3, 0.99]\n",
    "# num_total_layers = [4]\n",
    "# losses = ['categorical_hinge']\n",
    "# val_sizes = [0.4]\n",
    "# last_layer_sizes = [20]\n",
    "# last_layer_drop_sizes = [0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_split(test_size_input):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size_input, random_state=42)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def generate_csv(model, x_data, name):\n",
    "    ids = [i for i, _ in enumerate(x_data)]\n",
    "    ids = np.array(ids)\n",
    "    \n",
    "    predictions = model.predict(x_data).flatten()\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['id'] = ids\n",
    "    df['target'] = predictions\n",
    "    df.to_csv(name + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(activation, dropout, num_total_layer, last_layer_size, last_layer_drop_size):\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(len(x[0]),)))\n",
    "    model.add(Dense(300))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    num_layers_still_needed = num_total_layer - 2 # We manually create first and last\n",
    "        \n",
    "    step_size = int((300 - last_layer_size) / (num_layers_still_needed + 1))\n",
    "        \n",
    "    count = 300\n",
    "\n",
    "    \n",
    "    for i in range(num_layers_still_needed):\n",
    "        \n",
    "        count -= step_size\n",
    "\n",
    "        model.add(Dense(count))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(activation))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(last_layer_size))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(last_layer_drop_size))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class Stat_Collector (Callback):\n",
    "    def __init__(self, x_train, y_train, x_test, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.aucrocs_train = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        # Make aucroc prediction for training data\n",
    "        y_pred_train = self.model.predict(x_train)\n",
    "        y_pred_train = y_pred_train.flatten()\n",
    "        self.aucrocs_train.append(roc_auc_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, activation, dropout, num_total_layer, loss, val_size, last_layer_size, last_layer_drop_size, x_train, y_train):\n",
    "#     stat_collect = Stat_Collector(x_train, y_train)\n",
    "\n",
    "    model.compile(loss=loss,optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    fit = model.fit(x_train, y_train, batch_size=32, nb_epoch=23, verbose=1)\n",
    "    \n",
    "    x_actual_test = np.load(\"data/x_test.npy\")\n",
    "    \n",
    "    generate_csv(model, x_actual_test, \"submissions/nuro_net_test\")\n",
    "\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     num_epoch = len(23)\n",
    "\n",
    "#     # Plot training & validation accuracy values\n",
    "#     plt.plot([x + 1 for x in range(num_epoch)], stat_collect.aucrocs_train, label=\"Train AUCROC\")\n",
    "#     plt.plot([x + 1 for x in range(num_epoch)], stat_collect.aucrocs_val, label=\"Test AUCROC\")\n",
    "\n",
    "#     plt.xlabel('Test AUCROC maximized at epoch = {} with AUCROC = {}'.format(np.argmax(stat_collect.aucrocs_val) + 1, stat_collect.aucrocs_val[np.argmax(stat_collect.aucrocs_val)]))\n",
    "#     plt.ylabel('AUCROC')\n",
    "#     plt.title('Epochs vs. AUCROC')\n",
    "#     plt.legend(['Train', 'Test'], loc='upper left')\n",
    "#     plt.savefig(\"plots/nani\")\n",
    "#     plt.show()\n",
    "#     global best_aucroc\n",
    "#     if best_aucroc < stat_collect.aucrocs_val[np.argmax(stat_collect.aucrocs_val)]:\n",
    "#         best_aucroc = stat_collect.aucrocs_val[np.argmax(stat_collect.aucrocs_val)]\n",
    "#         with open(\"plots/best_plot.txt\", \"a+\") as f:\n",
    "#             f.write(\"plots/activation:{}_dropout:{}_numlayer:{}_loss:{}_valsize:{}_lastlayersize:{}_lastlayerdrop:{} \\n Performance: {}\\n\".format(activation, dropout, num_total_layer, loss, val_size, last_layer_size, last_layer_drop_size, stat_collect.aucrocs_val[np.argmax(stat_collect.aucrocs_val)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/23\n",
      "64667/64667 [==============================] - 9s 142us/step - loss: 0.5213 - acc: 0.7503\n",
      "Epoch 2/23\n",
      "64667/64667 [==============================] - 7s 110us/step - loss: 0.4932 - acc: 0.7636\n",
      "Epoch 3/23\n",
      "64667/64667 [==============================] - 7s 103us/step - loss: 0.4889 - acc: 0.7666\n",
      "Epoch 4/23\n",
      "64667/64667 [==============================] - 8s 117us/step - loss: 0.4859 - acc: 0.7684\n",
      "Epoch 5/23\n",
      "64667/64667 [==============================] - 8s 124us/step - loss: 0.4832 - acc: 0.7695\n",
      "Epoch 6/23\n",
      "64667/64667 [==============================] - 8s 121us/step - loss: 0.4832 - acc: 0.7704\n",
      "Epoch 7/23\n",
      "64667/64667 [==============================] - 6s 94us/step - loss: 0.4808 - acc: 0.7716\n",
      "Epoch 8/23\n",
      "64667/64667 [==============================] - 6s 95us/step - loss: 0.4797 - acc: 0.7726\n",
      "Epoch 9/23\n",
      "64667/64667 [==============================] - 6s 98us/step - loss: 0.4776 - acc: 0.7724\n",
      "Epoch 10/23\n",
      "64667/64667 [==============================] - 7s 102us/step - loss: 0.4770 - acc: 0.7745\n",
      "Epoch 11/23\n",
      "64667/64667 [==============================] - 7s 101us/step - loss: 0.4754 - acc: 0.7733\n",
      "Epoch 12/23\n",
      "64667/64667 [==============================] - 6s 92us/step - loss: 0.4756 - acc: 0.7743\n",
      "Epoch 13/23\n",
      "64667/64667 [==============================] - 6s 95us/step - loss: 0.4754 - acc: 0.7742\n",
      "Epoch 14/23\n",
      "64667/64667 [==============================] - 6s 94us/step - loss: 0.4741 - acc: 0.7748\n",
      "Epoch 15/23\n",
      "64667/64667 [==============================] - 6s 92us/step - loss: 0.4727 - acc: 0.7767\n",
      "Epoch 16/23\n",
      "64667/64667 [==============================] - 6s 101us/step - loss: 0.4729 - acc: 0.7773\n",
      "Epoch 17/23\n",
      "64667/64667 [==============================] - 6s 96us/step - loss: 0.4730 - acc: 0.7777\n",
      "Epoch 18/23\n",
      "64667/64667 [==============================] - 8s 117us/step - loss: 0.4717 - acc: 0.7780\n",
      "Epoch 19/23\n",
      "64667/64667 [==============================] - 7s 104us/step - loss: 0.4714 - acc: 0.7783\n",
      "Epoch 20/23\n",
      "64667/64667 [==============================] - 7s 103us/step - loss: 0.4728 - acc: 0.7756\n",
      "Epoch 21/23\n",
      "64667/64667 [==============================] - 7s 103us/step - loss: 0.4708 - acc: 0.7783\n",
      "Epoch 22/23\n",
      "64667/64667 [==============================] - 7s 116us/step - loss: 0.4718 - acc: 0.7780\n",
      "Epoch 23/23\n",
      "64667/64667 [==============================] - 9s 134us/step - loss: 0.4699 - acc: 0.7794\n"
     ]
    }
   ],
   "source": [
    "for activation in activations:\n",
    "    for dropout in dropouts:\n",
    "        for num_total_layer in num_total_layers:\n",
    "            for loss in losses:\n",
    "                for val_size in val_sizes:\n",
    "                    for last_layer_size in last_layer_sizes:\n",
    "                        for last_layer_drop_size in last_layer_drop_sizes:\n",
    "#                             x_train, x_test, y_train, y_test = get_correct_split(val_size)\n",
    "#                             print(activation, dropout, num_total_layer, loss, val_size, last_layer_size, last_layer_drop_size)\n",
    "                            model = create_model(activation, dropout, num_total_layer, last_layer_size, last_layer_drop_size)\n",
    "                            train_model(model, activation, dropout, num_total_layer, loss, val_size, last_layer_size, last_layer_drop_size, x, y)\n",
    "#                             model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_csv(model, x_data, name):\n",
    "#     ids = [i for i, _ in enumerate(x_data)]\n",
    "#     ids = np.array(ids)\n",
    "    \n",
    "#     predictions = model.predict(x_data).flatten()\n",
    "    \n",
    "#     df = pd.DataFrame()\n",
    "#     df['id'] = ids\n",
    "#     df['target'] = predictions\n",
    "#     df.to_csv(name + \".csv\", index=False)\n",
    "\n",
    "# #     print df.head()\n",
    "\n",
    "# x_actual_test = np.load(\"data/x_test.npy\")\n",
    "\n",
    "# generate_csv(model, x_actual_test, \"submissions/third_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
